{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Note\n",
        "\n",
        "- Do not restart the environment until you reach the NumPy installation command.\n",
        "\n",
        "- Note that if you are using a free Google Colab environment, you will experience frequent voting while the server is running."
      ],
      "metadata": {
        "id": "zynjlTjqI0tY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TwOZu3tkEqTV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1a989d22-52a7-4860-a4cb-d09c6c11ddf3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping onnxruntime as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping onnxruntime-gpu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "\u001b[33mWARNING: Skipping insightface as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4.tar.gz (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "Looking in indexes: https://pypi.org/simple, https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
            "Collecting onnxruntime-gpu==1.18.0\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/onnxruntime-gpu/1.18/onnxruntime_gpu-1.18.0-cp312-cp312-manylinux_2_28_x86_64.whl (200.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.6/200.6 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting coloredlogs (from onnxruntime-gpu==1.18.0)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/coloredlogs/15.0.1/coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.18.0) (25.9.23)\n",
            "Collecting numpy>=1.26.0 (from onnxruntime-gpu==1.18.0)\n",
            "  Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.18.0) (25.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.18.0) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime-gpu==1.18.0) (1.13.3)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu==1.18.0)\n",
            "  Downloading https://aiinfra.pkgs.visualstudio.com/2692857e-05ef-43b4-ba9c-ccf1c22c437c/_packaging/9387c3aa-d9ad-4513-968c-383f6f7f53b8/pypi/download/humanfriendly/10/humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime-gpu==1.18.0) (1.3.0)\n",
            "Downloading numpy-2.3.5-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m113.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, humanfriendly, coloredlogs, onnxruntime-gpu\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.3.5 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 2.3.5 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 numpy-2.3.5 onnxruntime-gpu-1.18.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "6288c98047c04db1af9e34aa5bace979"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting insightface==0.6.0\n",
            "  Downloading insightface-0.6.tar.gz (433 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/433.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m433.9/433.9 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (2.3.5)\n",
            "Collecting onnx (from insightface==0.6.0)\n",
            "  Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (2.32.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (3.10.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (11.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (1.6.1)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (0.25.2)\n",
            "Requirement already satisfied: easydict in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (1.13)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (3.0.12)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (2.0.8)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.12/dist-packages (from insightface==0.6.0) (3.16.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations->insightface==0.6.0) (6.0.3)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations->insightface==0.6.0) (2.11.10)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.12/dist-packages (from albumentations->insightface==0.6.0) (0.0.24)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0.80 in /usr/local/lib/python3.12/dist-packages (from albumentations->insightface==0.6.0) (4.12.0.88)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations->insightface==0.6.0) (4.2.3)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.12/dist-packages (from albucore==0.0.24->albumentations->insightface==0.6.0) (6.5.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->insightface==0.6.0) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->insightface==0.6.0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->insightface==0.6.0) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->insightface==0.6.0) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->insightface==0.6.0) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->insightface==0.6.0) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->insightface==0.6.0) (2.9.0.post0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx->insightface==0.6.0) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx->insightface==0.6.0) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx->insightface==0.6.0) (0.5.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prettytable->insightface==0.6.0) (0.2.14)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->insightface==0.6.0) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->insightface==0.6.0) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->insightface==0.6.0) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->insightface==0.6.0) (2025.10.5)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image->insightface==0.6.0) (3.5)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image->insightface==0.6.0) (2.37.2)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image->insightface==0.6.0) (2025.10.16)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->insightface==0.6.0) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->insightface==0.6.0) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->insightface==0.6.0) (3.6.0)\n",
            "Collecting numpy (from insightface==0.6.0)\n",
            "  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.0/62.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->insightface==0.6.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->insightface==0.6.0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations->insightface==0.6.0) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->insightface==0.6.0) (1.17.0)\n",
            "Downloading onnx-1.19.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: insightface\n",
            "  Building wheel for insightface (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for insightface: filename=insightface-0.6-cp312-cp312-linux_x86_64.whl size=1072160 sha256=104a1515a99d09ac80c66c164a8e158bab0687ce925e3efd4a6a79fff19aa8a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/3c/64/416d37f4a51a46d32184a4871be33a7369a4c510d67f1443be\n",
            "Successfully built insightface\n",
            "Installing collected packages: numpy, onnx, insightface\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.3.5\n",
            "    Uninstalling numpy-2.3.5:\n",
            "      Successfully uninstalled numpy-2.3.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed insightface-0.6 numpy-2.2.6 onnx-1.19.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "77568c01d68b4eef992cac1b02d0c7af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 41 not upgraded.\n",
            "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
            "built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
            "configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
            "libavutil      56. 70.100 / 56. 70.100\n",
            "libavcodec     58.134.100 / 58.134.100\n",
            "libavformat    58. 76.100 / 58. 76.100\n",
            "libavdevice    58. 13.100 / 58. 13.100\n",
            "libavfilter     7.110.100 /  7.110.100\n",
            "libswscale      5.  9.100 /  5.  9.100\n",
            "libswresample   3.  9.100 /  3.  9.100\n",
            "libpostproc    55.  9.100 / 55.  9.100\n"
          ]
        }
      ],
      "source": [
        "# Uninstall existing conflicting packages if they are already installed\n",
        "!pip uninstall -y onnxruntime onnxruntime-gpu numpy insightface\n",
        "\n",
        "# Install compatible numpy version first for onnxruntime\n",
        "!pip install numpy==1.24.4\n",
        "\n",
        "# Install onnxruntime-gpu with extra index URL\n",
        "!pip install onnxruntime-gpu==1.18.0 --extra-index-url https://aiinfra.pkgs.visualstudio.com/PublicPackages/_packaging/onnxruntime-cuda-12/pypi/simple/\n",
        "\n",
        "# Install an older insightface version compatible with numpy 1.x\n",
        "!pip install insightface==0.6.0\n",
        "\n",
        "# Install ffmpeg\n",
        "!apt-get install -y ffmpeg\n",
        "!ffmpeg -version\n",
        "!mkdir deepfakecollab"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EG4l7BC_pohY",
        "outputId": "1448fe51-b71d-46b2-a65e-6f22488ebb5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.2.6\n",
            "Uninstalling numpy-2.2.6:\n",
            "  Successfully uninstalled numpy-2.2.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"numpy<2\" # Restart the runtime environment after running this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        },
        "id": "UxwRBi2gpqYJ",
        "outputId": "9001809e-106a-439f-d1c8-a477308ceed8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy<2\n",
            "  Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Using cached numpy-1.26.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.0 MB)\n",
            "Installing collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "opencv-python-headless 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "shap 0.50.0 requires numpy>=2, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\n",
            "jax 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "jaxlib 0.7.2 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "pytensor 2.35.1 requires numpy>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
            "opencv-contrib-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "d09fd7bdd73441c6b7edd6a4c1c4a460"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd deepfakecollab\n",
        "!mkdir deepfakecollab/Scripts"
      ],
      "metadata": {
        "id": "w_PI8ezaEt0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!touch deepfakecollab/Scripts/get_ngrok.sh\n",
        "getfrs = \"\"\"#!/usr/bin/env bash\n",
        "# Check if frpc is installed\n",
        "command -v frpc >/dev/null 2>&1\n",
        "if [[ $? -ne 0 ]]; then\n",
        "    echo \"ngrok is not found, installing...\"\n",
        "    wget -q -nc https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-amd64.tgz\n",
        "    tar -xzf ngrok-v3-stable-linux-amd64.tgz\n",
        "    echo \"Done!\"\n",
        "fi\"\"\"\n",
        "with open('deepfakecollab/Scripts/get_ngrok.sh', 'w') as f:\n",
        "    f.write(getfrs)\n",
        "!touch deepfakecollab/Scripts/open_tunnel_ngrok.sh\n",
        "getfrs = \"\"\"#!/usr/bin/env bash\n",
        "cmd=\"./ngrok start --all --config ngrok.conf\"\n",
        "kill -9 $(ps aux | grep $cmd | awk '{print $2}') 2> /dev/null\n",
        "echo Opening tunnel\n",
        "$cmd\"\"\"\n",
        "with open('deepfakecollab/Scripts/open_tunnel_ngrok.sh', 'w') as f:\n",
        "    f.write(getfrs)\n",
        "!chmod +x deepfakecollab/Scripts/get_ngrok.sh\n",
        "!chmod +x deepfakecollab/Scripts/open_tunnel_ngrok.sh\n",
        "!deepfakecollab/Scripts/get_ngrok.sh"
      ],
      "metadata": {
        "id": "AS_ygzeqFBfO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dbd1e5c-19ef-40a3-bb10-2a122992e049"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ngrok is not found, installing...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your region here in quotes\n",
        "# Paste your authtoken here in quotes\n",
        "authtoken = \"\"\n",
        "region = \"us\""
      ],
      "metadata": {
        "id": "SwnIvEjFFEXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This link is working, the previous one was down\n",
        "\n",
        "!mkdir -p deepfakecollab/Model\n",
        "!wget https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx -P /content/deepfakecollab/Model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kb53gXWmrPTI",
        "outputId": "5a25ec96-4192-4fd1-9bd3-f2f637fe1114"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-18 03:50:24--  https://huggingface.co/hacksider/deep-live-cam/resolve/main/inswapper_128_fp16.onnx\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.38, 3.168.73.111, 3.168.73.106, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/651037cdc3b9d8d1eb5987e4/3df5a109d36c1cfcd2de7cf4744eba3997b15d3b5078e0da0a8b3034de848e0a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251118T035025Z&X-Amz-Expires=3600&X-Amz-Signature=92b3e53a40ef0ecf88de6ba077d706f4e15f8c4fc7e8a8152c5b21e3f55554f6&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27inswapper_128_fp16.onnx%3B+filename%3D%22inswapper_128_fp16.onnx%22%3B&x-id=GetObject&Expires=1763441425&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MzQ0MTQyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTEwMzdjZGMzYjlkOGQxZWI1OTg3ZTQvM2RmNWExMDlkMzZjMWNmY2QyZGU3Y2Y0NzQ0ZWJhMzk5N2IxNWQzYjUwNzhlMGRhMGE4YjMwMzRkZTg0OGUwYSoifV19&Signature=LB0rLGsSw9GOB2KqeO%7EDvg-RhARuuZEvd3bpcGHtKer1s-98gzaoAKFeB0cwcVdtxglgNI5frrKxW3uXXBJYhTbvjAWko2pTkzjNCotQJlVfEDfrYN0RAHsKOvHPlwNSx2TpGCsQf%7ETE30cyEAYS4GLGidn56PWHV%7EEiAPUMRU-9x7HrFgpxNfPRXnSYSVefk5zzPRBsD68V%7ErHASZAtF9YQg5Cdxj2qo0nXDR5WAeEkDbyfo7QiRAr40RcUwCXAUE9rat%7EYvG7k3RB6TwsvDfKMsQWuz6e36csmRw2kDDkZIJQvv0keZOWMjlEA0qKcWiKk7HCiy2l4aIxlBsrZWA__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-11-18 03:50:25--  https://cas-bridge.xethub.hf.co/xet-bridge-us/651037cdc3b9d8d1eb5987e4/3df5a109d36c1cfcd2de7cf4744eba3997b15d3b5078e0da0a8b3034de848e0a?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251118T035025Z&X-Amz-Expires=3600&X-Amz-Signature=92b3e53a40ef0ecf88de6ba077d706f4e15f8c4fc7e8a8152c5b21e3f55554f6&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27inswapper_128_fp16.onnx%3B+filename%3D%22inswapper_128_fp16.onnx%22%3B&x-id=GetObject&Expires=1763441425&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MzQ0MTQyNX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTEwMzdjZGMzYjlkOGQxZWI1OTg3ZTQvM2RmNWExMDlkMzZjMWNmY2QyZGU3Y2Y0NzQ0ZWJhMzk5N2IxNWQzYjUwNzhlMGRhMGE4YjMwMzRkZTg0OGUwYSoifV19&Signature=LB0rLGsSw9GOB2KqeO%7EDvg-RhARuuZEvd3bpcGHtKer1s-98gzaoAKFeB0cwcVdtxglgNI5frrKxW3uXXBJYhTbvjAWko2pTkzjNCotQJlVfEDfrYN0RAHsKOvHPlwNSx2TpGCsQf%7ETE30cyEAYS4GLGidn56PWHV%7EEiAPUMRU-9x7HrFgpxNfPRXnSYSVefk5zzPRBsD68V%7ErHASZAtF9YQg5Cdxj2qo0nXDR5WAeEkDbyfo7QiRAr40RcUwCXAUE9rat%7EYvG7k3RB6TwsvDfKMsQWuz6e36csmRw2kDDkZIJQvv0keZOWMjlEA0qKcWiKk7HCiy2l4aIxlBsrZWA__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.33.67.126, 13.33.67.95, 13.33.67.21, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.33.67.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277288649 (264M)\n",
            "Saving to: ‘/content/deepfakecollab/Model/inswapper_128_fp16.onnx’\n",
            "\n",
            "inswapper_128_fp16. 100%[===================>] 264.44M   250MB/s    in 1.1s    \n",
            "\n",
            "2025-11-18 03:50:26 (250 MB/s) - ‘/content/deepfakecollab/Model/inswapper_128_fp16.onnx’ saved [277288649/277288649]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth -P /content/deepfakecollab/Model"
      ],
      "metadata": {
        "id": "4tF6VZX8FH5d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f38a264f-28fc-41ce-ca83-a6b1839b6c4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-18 03:50:28--  https://huggingface.co/hacksider/deep-live-cam/resolve/main/GFPGANv1.4.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.168.73.38, 3.168.73.111, 3.168.73.106, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.168.73.38|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/651037cdc3b9d8d1eb5987e4/7effea926d0d68c45f31adcfc8ab34c7316101df430eaf824dc94e89b9f55fed?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251118T033532Z&X-Amz-Expires=3600&X-Amz-Signature=f3ac0aa875e3c4e69e231b9fa530b118291702045d9d148b01205dd212e3667d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27GFPGANv1.4.pth%3B+filename%3D%22GFPGANv1.4.pth%22%3B&x-id=GetObject&Expires=1763440532&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MzQ0MDUzMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTEwMzdjZGMzYjlkOGQxZWI1OTg3ZTQvN2VmZmVhOTI2ZDBkNjhjNDVmMzFhZGNmYzhhYjM0YzczMTYxMDFkZjQzMGVhZjgyNGRjOTRlODliOWY1NWZlZCoifV19&Signature=Rbxaq2m97eo2ZgLd2XhsohT-pe3fgdlZ7IsjgeCckTAKpVBCG5H0GFOFxvQ8IozDsCoWawFhMIYvgUawvMGLrxr9byZ%7ESrhea4UPYJKvEYNw2Gv7qf75Q6wbe-ev0YGF6OM9FY%7EjBejR9forlu85O20YOlumR1AVHq7pp-Jt8a%7EZkrT9U8aOn1vtIQQgpNMJ3EzcxKuy-ysyp2j%7EqVyWLjH8CQF5hYZWhcFYgMB%7EjHBwYgq%7ELoW2OX3k9%7EaB5JhkoiGD4%7ENle92HhyYC3JBB5AcrBttPpuVvGg3tPhLUFOJH1LWD2R2avPXPSeHusOGb4Hv78bdSMxMD%7ECZEdtkDvw__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-11-18 03:50:28--  https://cas-bridge.xethub.hf.co/xet-bridge-us/651037cdc3b9d8d1eb5987e4/7effea926d0d68c45f31adcfc8ab34c7316101df430eaf824dc94e89b9f55fed?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251118%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251118T033532Z&X-Amz-Expires=3600&X-Amz-Signature=f3ac0aa875e3c4e69e231b9fa530b118291702045d9d148b01205dd212e3667d&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27GFPGANv1.4.pth%3B+filename%3D%22GFPGANv1.4.pth%22%3B&x-id=GetObject&Expires=1763440532&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2MzQ0MDUzMn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NTEwMzdjZGMzYjlkOGQxZWI1OTg3ZTQvN2VmZmVhOTI2ZDBkNjhjNDVmMzFhZGNmYzhhYjM0YzczMTYxMDFkZjQzMGVhZjgyNGRjOTRlODliOWY1NWZlZCoifV19&Signature=Rbxaq2m97eo2ZgLd2XhsohT-pe3fgdlZ7IsjgeCckTAKpVBCG5H0GFOFxvQ8IozDsCoWawFhMIYvgUawvMGLrxr9byZ%7ESrhea4UPYJKvEYNw2Gv7qf75Q6wbe-ev0YGF6OM9FY%7EjBejR9forlu85O20YOlumR1AVHq7pp-Jt8a%7EZkrT9U8aOn1vtIQQgpNMJ3EzcxKuy-ysyp2j%7EqVyWLjH8CQF5hYZWhcFYgMB%7EjHBwYgq%7ELoW2OX3k9%7EaB5JhkoiGD4%7ENle92HhyYC3JBB5AcrBttPpuVvGg3tPhLUFOJH1LWD2R2avPXPSeHusOGb4Hv78bdSMxMD%7ECZEdtkDvw__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 13.33.67.126, 13.33.67.95, 13.33.67.21, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|13.33.67.126|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 348632874 (332M)\n",
            "Saving to: ‘/content/deepfakecollab/Model/GFPGANv1.4.pth’\n",
            "\n",
            "GFPGANv1.4.pth      100%[===================>] 332.48M  43.3MB/s    in 4.6s    \n",
            "\n",
            "2025-11-18 03:50:32 (71.6 MB/s) - ‘/content/deepfakecollab/Model/GFPGANv1.4.pth’ saved [348632874/348632874]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import onnxruntime as rt"
      ],
      "metadata": {
        "id": "22CdJJBroRJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ort_session = rt.InferenceSession(\n",
        "    \"/content/deepfakecollab/Model/inswapper_128_fp16.onnx\",\n",
        "    providers=[\"TensorrtExecutionProvider\", \"CUDAExecutionProvider\", \"CPUExecutionProvider\"]\n",
        ")\n",
        "# Verify the active provider again\n",
        "print(\"Active providers:\", ort_session.get_providers())"
      ],
      "metadata": {
        "id": "W6XHTT-pFImd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ff8f423-4c97-483d-c9b9-90417c1a5b6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*************** EP Error ***************\n",
            "EP Error /onnxruntime_src/onnxruntime/python/onnxruntime_pybind_state.cc:456 void onnxruntime::python::RegisterTensorRTPluginsAsCustomOps(onnxruntime::python::PySessionOptions&, const ProviderOptions&) Please install TensorRT libraries as mentioned in the GPU requirements page, make sure they're in the PATH or LD_LIBRARY_PATH, and that your GPU is supported.\n",
            " when using ['TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']\n",
            "Falling back to ['CUDAExecutionProvider', 'CPUExecutionProvider'] and retrying.\n",
            "****************************************\n",
            "Active providers: ['CPUExecutionProvider']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime import get_device\n",
        "print(\"ONNX Runtime is using:\", get_device())"
      ],
      "metadata": {
        "id": "xFTRn8cfFPe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970058f8-8f17-41aa-815d-406329a1cb85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ONNX Runtime is using: GPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /root/.insightface/models"
      ],
      "metadata": {
        "id": "owu-BWZEtKix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# insightface was giving an error when downloading the models from the library, so it's done manually.\n",
        "\n",
        "!wget https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip -O /root/.insightface/models/buffalo_l.zip\n",
        "\n",
        "!unzip -o /root/.insightface/models/buffalo_l.zip -d /root/.insightface/models/buffalo_l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wka21xF6tDal",
        "outputId": "ccf5c6c6-8a66-43a8-8a29-f36788f9403e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-11-18 03:50:44--  https://github.com/deepinsight/insightface/releases/download/v0.7/buffalo_l.zip\n",
            "Resolving github.com (github.com)... 140.82.114.3\n",
            "Connecting to github.com (github.com)|140.82.114.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/102057483/8ae2cf05-2fe7-45cc-addd-2fc11120dcd1?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-18T04%3A27%3A41Z&rscd=attachment%3B+filename%3Dbuffalo_l.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-18T03%3A27%3A34Z&ske=2025-11-18T04%3A27%3A41Z&sks=b&skv=2018-11-09&sig=upfqjo50Qy19vlPzbONKDNWQleGsXaqpaW7zSynkthc%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MzQ0MTM0NywibmJmIjoxNzYzNDM3NzQ3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.COWkjVkjerZy4llGn19q48HE5DG6h5qRUwrSr8O6weY&response-content-disposition=attachment%3B%20filename%3Dbuffalo_l.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2025-11-18 03:50:44--  https://release-assets.githubusercontent.com/github-production-release-asset/102057483/8ae2cf05-2fe7-45cc-addd-2fc11120dcd1?sp=r&sv=2018-11-09&sr=b&spr=https&se=2025-11-18T04%3A27%3A41Z&rscd=attachment%3B+filename%3Dbuffalo_l.zip&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2025-11-18T03%3A27%3A34Z&ske=2025-11-18T04%3A27%3A41Z&sks=b&skv=2018-11-09&sig=upfqjo50Qy19vlPzbONKDNWQleGsXaqpaW7zSynkthc%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc2MzQ0MTM0NywibmJmIjoxNzYzNDM3NzQ3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.COWkjVkjerZy4llGn19q48HE5DG6h5qRUwrSr8O6weY&response-content-disposition=attachment%3B%20filename%3Dbuffalo_l.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 288621354 (275M) [application/octet-stream]\n",
            "Saving to: ‘/root/.insightface/models/buffalo_l.zip’\n",
            "\n",
            "/root/.insightface/ 100%[===================>] 275.25M   298MB/s    in 0.9s    \n",
            "\n",
            "2025-11-18 03:50:45 (298 MB/s) - ‘/root/.insightface/models/buffalo_l.zip’ saved [288621354/288621354]\n",
            "\n",
            "Archive:  /root/.insightface/models/buffalo_l.zip\n",
            "  inflating: /root/.insightface/models/buffalo_l/genderage.onnx  \n",
            "  inflating: /root/.insightface/models/buffalo_l/2d106det.onnx  \n",
            "  inflating: /root/.insightface/models/buffalo_l/det_10g.onnx  \n",
            "  inflating: /root/.insightface/models/buffalo_l/1k3d68.onnx  \n",
            "  inflating: /root/.insightface/models/buffalo_l/w600k_r50.onnx  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import socket\n",
        "import cv2\n",
        "import numpy as np\n",
        "import insightface\n",
        "import threading\n",
        "import torch\n",
        "import onnxruntime\n",
        "from typing import Any\n",
        "from insightface.app.common import Face\n",
        "import matplotlib.pyplot as plt\n",
        "# Check if CUDA is available\n",
        "if 'CUDAExecutionProvider' in onnxruntime.get_available_providers():\n",
        "    providers = ['CUDAExecutionProvider']\n",
        "elif 'TensorrtExecutionProvider'  in onnxruntime.get_available_providers():\n",
        "    providers = ['TensorrtExecutionProvider']\n",
        "else:\n",
        "    providers = ['CPUExecutionProvider']\n",
        "print(providers)\n",
        "FACE_SWAPPER = None\n",
        "FACE_ANALYSER = insightface.app.FaceAnalysis(name='buffalo_l', providers=providers)\n",
        "FACE_ANALYSER.prepare(ctx_id=0, det_size=(640, 640))\n",
        "THREAD_LOCK = threading.Lock()\n",
        "Frame = np.ndarray[Any, Any]\n",
        "def get_face_swapper() -> Any:\n",
        "    global FACE_SWAPPER\n",
        "    with THREAD_LOCK:\n",
        "        if FACE_SWAPPER is None:\n",
        "            model_path = \"/content/deepfakecollab/Model/inswapper_128_fp16.onnx\"\n",
        "            FACE_SWAPPER = insightface.model_zoo.get_model(model_path, providers=['CUDAExecutionProvider'])\n",
        "    return FACE_SWAPPER\n",
        "def swap_face(source_face: Face, target_face: Face, temp_frame: Frame) -> Frame:\n",
        "    return get_face_swapper().get(temp_frame, target_face, source_face, paste_back=True)\n",
        "def get_face_analyser() -> Any:\n",
        "    #FACE_ANALYSER.prepare(ctx_id=0, det_size=(640, 640))\n",
        "    return FACE_ANALYSER\n",
        "def get_one_face(frame: Frame) -> Any:\n",
        "    face = get_face_analyser().get(frame)\n",
        "    try:\n",
        "        return min(face, key=lambda x: x.bbox[0])\n",
        "    except ValueError:\n",
        "        return None\n",
        "def get_many_faces(frame: Frame) -> Any:\n",
        "    try:\n",
        "        return get_face_analyser().get(frame)\n",
        "    except IndexError:\n",
        "        return None\n",
        "def process_frame(source_face: Face, temp_frame: Frame,manyface: bool) -> Frame:\n",
        "    if manyface:\n",
        "        many_faces = get_many_faces(temp_frame)\n",
        "        if many_faces:\n",
        "            for target_face in many_faces:\n",
        "                temp_frame = swap_face(source_face, target_face, temp_frame)\n",
        "    else:\n",
        "        target_face = get_one_face(temp_frame)\n",
        "        if target_face:\n",
        "            temp_frame = swap_face(source_face, target_face, temp_frame)\n",
        "    return temp_frame\n",
        "# Input and output ports for communication\n",
        "local_in_source = 5555\n",
        "local_in_temp = 5556\n",
        "local_out_frame = 5557"
      ],
      "metadata": {
        "id": "qpXjHwfmFZgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dd35deb-980a-4e9a-e660-2efe85f1e87c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['CUDAExecutionProvider']\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/1k3d68.onnx landmark_3d_68 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/2d106det.onnx landmark_2d_106 ['None', 3, 192, 192] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/det_10g.onnx detection [1, 3, '?', '?'] 127.5 128.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/genderage.onnx genderage ['None', 3, 96, 96] 0.0 1.0\n",
            "Applied providers: ['CPUExecutionProvider'], with options: {'CPUExecutionProvider': {}}\n",
            "find model: /root/.insightface/models/buffalo_l/w600k_r50.onnx recognition ['None', 3, 112, 112] 127.5 127.5\n",
            "set det-size: (640, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zmq\n",
        "import threading\n",
        "import cv2\n",
        "import numpy as np\n",
        "import msgpack\n",
        "import queue\n",
        "import time\n",
        "import zlib\n",
        "from tqdm import tqdm\n",
        "import subprocess\n",
        "from collections import deque\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "from collections import deque\n",
        "\n",
        "# Initialize the executor with the number of workers\n",
        "executor = ThreadPoolExecutor(max_workers=4)  # Adjust based on your CPU cores\n",
        "# Batch size - tune this based on available resources and processing time per frame\n",
        "BATCH_SIZE = 120\n",
        "# Holds future results for parallel processing\n",
        "future_to_batch = {}\n",
        "#import matplotlib.pyplot as plt\n",
        "def create_demo_image():\n",
        "    # Create a demo image (e.g., a solid color or pattern)\n",
        "    demo_image = np.zeros((540, 960, 3), dtype=np.uint8)  # Black image\n",
        "    cv2.putText(demo_image, 'Demo Image', (50, 240), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
        "    return demo_image\n",
        "def pull_socket(local_in_port):\n",
        "    context = zmq.Context()\n",
        "    socket = context.socket(zmq.REP)\n",
        "    socket.setsockopt(zmq.RCVHWM, 100000)\n",
        "    socket.setsockopt(zmq.LINGER, 0)\n",
        "    address = f\"tcp://127.0.0.1:{local_in_port}\"\n",
        "    socket.bind(address)  # Binding to a different local port\n",
        "    print(f\"PULL one socket bound to {address}\")\n",
        "    return socket\n",
        "# Compress image\n",
        "def compress_image(image, quality=95):\n",
        "    # Set the JPEG quality parameter\n",
        "    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]\n",
        "    # Encode the image as a JPEG\n",
        "    result, encimg = cv2.imencode('.jpg', image, encode_param)\n",
        "    if not result:\n",
        "        raise Exception(\"Image encoding failed\")\n",
        "    # Decode the encoded image back to an image format\n",
        "    decimg = cv2.imdecode(encimg, 1)\n",
        "    return decimg\n",
        "# Decompress image\n",
        "def decompress_image(encimg):\n",
        "    image = cv2.imdecode(np.frombuffer(encimg, np.uint8), cv2.IMREAD_COLOR)\n",
        "    return image\n",
        "# Global variables\n",
        "frames_array=deque(maxlen=2000)\n",
        "temp_frames_array=deque(maxlen=2000)\n",
        "source_frame = None\n",
        "is_manyFace = None\n",
        "frameSize = '960x540'\n",
        "fps = None\n",
        "# Process a batch of frames simultaneously\n",
        "# Function to process each frame\n",
        "def process_frame_parallel(source, frame, many_face):\n",
        "    return process_frame(get_one_face(source), frame, many_face)\n",
        "\n",
        "def pull_worker(pull_socket):\n",
        "    global source_frame,is_manyFace,frameSize,fps\n",
        "    while True:\n",
        "        try:\n",
        "            # Receive the JSON with total chunks\n",
        "            meta_data_json = pull_socket.recv_json()\n",
        "            #print(meta_data_json)\n",
        "            total_chunk = meta_data_json['total_chunk']\n",
        "            # Send acknowledgment for metadata\n",
        "            pull_socket.send_string(\"ACK\")\n",
        "            # Receive the array bytes\n",
        "            source_array_bytes =b''\n",
        "            for i in range(total_chunk):\n",
        "                chunk = pull_socket.recv()\n",
        "                source_array_bytes += chunk\n",
        "                pull_socket.send_string(f\"ACK {i + 1}/{total_chunk}\")\n",
        "            end_message = pull_socket.recv()\n",
        "            if end_message == b\"END\":\n",
        "                pull_socket.send_string(\"Final ACK\")\n",
        "\n",
        "            # Deserialize the bytes back to an ndarray\n",
        "            source_array = np.frombuffer(source_array_bytes, dtype=np.dtype(meta_data_json['dtype_source'])).reshape(meta_data_json['shape_source'])\n",
        "            source_frame = source_array\n",
        "            is_manyFace = meta_data_json['manyface']\n",
        "            frameSize =  meta_data_json['size']\n",
        "            fps = meta_data_json[\"fps\"]\n",
        "                #process_queue.put((\"source\", source_array))\n",
        "            break\n",
        "        except zmq.Again:\n",
        "            # Sleep briefly to avoid busy-waiting\n",
        "            time.sleep(0.01)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "def pull_worker_two(local_in_temp):\n",
        "    ffmpeg_receive_command =  [\n",
        "        'ffmpeg',\n",
        "        '-i',f'tcp://127.0.0.1:{local_in_temp}?listen&fifo_size=100000&overrun_nonfatal=1',\n",
        "        '-f','rawvideo',\n",
        "        '-pix_fmt','bgr24',\n",
        "        '-s','960x540',\n",
        "        'pipe:1'\n",
        "    ]\n",
        "    ffmpeg_receive_process = subprocess.Popen((ffmpeg_receive_command), stdout=subprocess.PIPE)\n",
        "    timefame =1/25\n",
        "    print(f\"InputStream ffmpeg bound from tcp://127.0.0.1:{local_in_temp}?listen\")\n",
        "    global source_frame,is_manyFace\n",
        "    future_to_frame = {}\n",
        "    while True:\n",
        "        try:\n",
        "            # Receive the JSON with total chunks\n",
        "            # Read decoded frame from FFmpeg\n",
        "            raw_frame = ffmpeg_receive_process.stdout.read(960 * 540 * 3)\n",
        "            if not raw_frame:\n",
        "                break\n",
        "            framex = np.frombuffer(raw_frame, dtype=np.uint8).reshape((540, 960, 3))\n",
        "            source_array = source_frame\n",
        "            is_many_face = is_manyFace\n",
        "            temp_frames_array.append(framex)\n",
        "            if source_frame is not None:\n",
        "                while len(temp_frames_array)>1:\n",
        "                    t_frames = temp_frames_array[-1]\n",
        "                    frames_array.append(t_frames)\n",
        "                    temp_frames_array.popleft()\n",
        "        except zmq.Again:\n",
        "            # Sleep briefly to avoid busy-waiting\n",
        "            time.sleep(0.01)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "def push_worker(local_out_frame):\n",
        "    source_array = None\n",
        "    temp_array = None\n",
        "    is_many_face = None\n",
        "    timefrate = 120\n",
        "    maxcount = 0\n",
        "    global source_frame,is_manyFace,frameSize,fps\n",
        "    print(f\"OutputStream ffmpeg bound to tcp://127.0.0.1:{local_out_frame}?listen\")\n",
        "    ffmpeg_encode_command = [\n",
        "        'ffmpeg',\n",
        "        '-fflags','+nobuffer',\n",
        "        '-flags', 'low_delay',\n",
        "        '-probesize', '32',\n",
        "        '-f', 'rawvideo',\n",
        "        '-pix_fmt', 'bgr24',\n",
        "        '-s', '960x540',\n",
        "        '-r', str(timefrate),\n",
        "        '-i', 'pipe:',\n",
        "        '-c:v', 'libx264',  # Enable encoding for lower bandwidth usage\n",
        "        '-preset', 'ultrafast',  # Fast encoding to reduce latency\n",
        "        '-tune', 'zerolatency',  # Low latency setting\n",
        "        '-g', '60',  # GOP size set to 1 to make each frame a keyframe\n",
        "        '-b:v', '3000k',  # Set bitrate to 1 Mbps\n",
        "        '-maxrate', '3000k',\n",
        "        '-bufsize', '5000k',\n",
        "        '-threads', '8',  # Enable threading\n",
        "        '-f', 'mpegts',\n",
        "        f'tcp://127.0.0.1:{local_out_frame}?listen&fifo_size=50000&overrun_nonfatal=1'\n",
        "    ]\n",
        "    timefame =1/timefrate\n",
        "    ffmpeg_encode_process = subprocess.Popen((ffmpeg_encode_command), stdin=subprocess.PIPE)\n",
        "    demo_image = create_demo_image()\n",
        "    frames_to_skip = 200  # Number of frames to skip to reduce delay\n",
        "    frame_count =0\n",
        "    wait_frame = 0\n",
        "    try:\n",
        "        while True:\n",
        "            # Get the processed array and metadata from the queue\n",
        "            if len(frames_array)>wait_frame:\n",
        "                if wait_frame<timefrate:\n",
        "                    wait_frame += 1\n",
        "                    #timefrate -=1\n",
        "\n",
        "                temp_array = frames_array[-1]#frames_array.popleft()#process_queue.get()\n",
        "                source_array = source_frame\n",
        "                is_many_face = is_manyFace\n",
        "                #framex = temp_array\n",
        "                #print(source_array,temp_array)\n",
        "                processed_array = process_frame(get_one_face(source_frame),temp_array,is_manyFace)\n",
        "                framex = processed_array\n",
        "                '''\n",
        "                if source_frame is not None: #and temp_array is not None:\n",
        "                    source_array = None\n",
        "                    temp_array = None\n",
        "                    is_many_face = None\n",
        "\n",
        "                '''\n",
        "            else:\n",
        "                framex = demo_image\n",
        "                # Write the frame to FFmpeg for encoding and streaming\n",
        "            ffmpeg_encode_process.stdin.write(framex.tobytes())\n",
        "            time.sleep(timefame)\n",
        "    finally:\n",
        "    #ffmpeg_receive_process.terminate()\n",
        "        ffmpeg_encode_process.terminate()\n",
        "# Create sockets\n",
        "pull_socket = pull_socket(local_in_source)\n",
        "# Run both workers in separate threads\n",
        "# Start the pull worker thread\n",
        "pull_thread = threading.Thread(target=pull_worker, args=(pull_socket,))\n",
        "pull_thread.start()\n",
        "# Start the push worker thread\n",
        "pull_thread_two = threading.Thread(target=pull_worker_two, args=(local_in_temp,))\n",
        "pull_thread_two.start()\n",
        "# Start the push worker thread\n",
        "push_thread = threading.Thread(target=push_worker,args=(local_out_frame,))\n",
        "push_thread.start()"
      ],
      "metadata": {
        "id": "MgkVeVl3FkIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2275f8c5-67df-4fb8-d762-c50e03fe14ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PULL one socket bound to tcp://127.0.0.1:5555\n",
            "OutputStream ffmpeg bound to tcp://127.0.0.1:5557?listen\n",
            "InputStream ffmpeg bound from tcp://127.0.0.1:5556?listen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from subprocess import Popen, PIPE\n",
        "import shlex\n",
        "import json\n",
        "import time\n",
        "def run_with_pipe(command):\n",
        "  commands = list(map(shlex.split,command.split(\"|\")))\n",
        "  ps = Popen(commands[0], stdout=PIPE, stderr=PIPE)\n",
        "  for command in commands[1:]:\n",
        "    ps = Popen(command, stdin=ps.stdout, stdout=PIPE, stderr=PIPE)\n",
        "  return ps.stdout.readlines()\n",
        "def get_tunnel_adresses():\n",
        "  info = run_with_pipe(\"curl http://localhost:4040/api/tunnels\")\n",
        "  assert info\n",
        "  info = json.loads(info[0])\n",
        "  for tunnel in info['tunnels']:\n",
        "    url = tunnel['public_url']\n",
        "    port = url.split(':')[-1]\n",
        "    local_port = tunnel['config']['addr'].split(':')[-1]\n",
        "    print(f'{url} -> {local_port} [{tunnel[\"name\"]}]')\n",
        "    if tunnel['name'] == 'input':\n",
        "      in_addr = url\n",
        "    elif tunnel['name'] == 'inputtwo':\n",
        "      in_addrtwo = url\n",
        "    elif tunnel['name'] == 'output':\n",
        "      out_addr = url\n",
        "    else:\n",
        "      print(f'unknown tunnel: {tunnel[\"name\"]}')\n",
        "  return in_addr,in_addrtwo, out_addr\n",
        "config =\\\n",
        "f\"\"\"\n",
        "version: 2\n",
        "authtoken: {authtoken}\n",
        "region: {region}\n",
        "console_ui: False\n",
        "tunnels:\n",
        "  input:\n",
        "    addr: {local_in_source}\n",
        "    proto: tcp\n",
        "  inputtwo:\n",
        "    addr: {local_in_temp}\n",
        "    proto: tcp\n",
        "  output:\n",
        "    addr: {local_out_frame}\n",
        "    proto: tcp\n",
        "\"\"\"\n",
        "with open('ngrok.conf', 'w') as f:\n",
        "  f.write(config)\n",
        "# (Re)Open tunnel\n",
        "ps = Popen('/content/deepfakecollab/Scripts/open_tunnel_ngrok.sh', stdout=PIPE, stderr=PIPE)\n",
        "time.sleep(3)\n",
        "# Get tunnel addresses\n",
        "try:\n",
        "  in_addr,in_addr_two, out_addr = get_tunnel_adresses()\n",
        "  print(\"Tunnel opened\")\n",
        "except Exception as e:\n",
        "  [print(l.decode(), end='') for l in ps.stdout.readlines()]\n",
        "  print(\"Something went wrong, reopen the tunnel\")"
      ],
      "metadata": {
        "id": "RHlZhBbIFrsV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10bdb8bf-cd1a-4d0c-8ff5-3e85021d8c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tcp://0.tcp.ngrok.io:13265 -> 5555 [input]\n",
            "tcp://6.tcp.ngrok.io:18663 -> 5556 [inputtwo]\n",
            "tcp://8.tcp.ngrok.io:18316 -> 5557 [output]\n",
            "Tunnel opened\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import subprocess\n",
        "# List of ports to free\n",
        "ports = [5555, 5556, 5557]\n",
        "for port in ports:\n",
        "    # Find the process ID (PID) using the port\n",
        "    result = subprocess.run(f\"lsof -t -i:{port}\", shell=True, capture_output=True, text=True)\n",
        "    pid = result.stdout.strip()\n",
        "\n",
        "    # Kill the process if it exists\n",
        "    if pid:\n",
        "        subprocess.run(f\"kill -9 {pid}\", shell=True)\n",
        "        print(f\"Terminated process on port {port} with PID {pid}\")\n",
        "    else:\n",
        "        print(f\"No process found on port {port}\")"
      ],
      "metadata": {
        "id": "9pMq09t3Fsd0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}